{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37b6829",
   "metadata": {},
   "source": [
    "# Assignment Title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f6246b",
   "metadata": {},
   "source": [
    "## Programming Assignment (40 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5dfa2a",
   "metadata": {},
   "source": [
    "The programming assignement will be an implementation of the task described in the assignment\n",
    "\n",
    "We will make sure you have enough scaffolding to build the code upon where you would only have to implement the interesting parts of the code\n",
    "\n",
    "### Evaluation\n",
    "The evaluation of the assignment will be done through test scripts that you would need to pass to get the points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71455cfd",
   "metadata": {},
   "source": [
    "## Written Assignment (60 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05348ced",
   "metadata": {},
   "source": [
    "Written assignment tests the understanding of the student for the assignment's task. We have split the writing into sections. You will need to write 1-2 paragraphs describing the sections. Please be concise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99841650",
   "metadata": {},
   "source": [
    "### In your own words, describe what the task is (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d049c34a",
   "metadata": {},
   "source": [
    "The task that was provided resolves around the Hidden Markov models and the Viterbi algorithm. The hidden markov model is the probabilistic model which is used to infer unobserved information from the observed states. HMM is a very powerful staterstical modelling tool used in the speech recognition.The model deals with the sequences of states and the assumption that the model takes is the next state is only dependent on the previous states rather than all the previous states. The model consists of the state to state transition and the state to observables which we call as the transition and the emission probability. The model also consist of the initial states from which the algorithm starts and the observable value is being reached. So the basic parts are as follows :-\n",
    "\n",
    "1. hidden states\n",
    "2. observation states or symbols\n",
    "3. transition from the initial state to intial hidden state probability distribution.\n",
    "4. transition to terminal state probability distribution.\n",
    "5. state transition probability distribution.\n",
    "6. state emission probability distribution.\n",
    "\n",
    "\n",
    "The HMM model uses the recursive approch to use all the the different possible sequences of hidden states by summing over all the joint probabilities. The complexity is calculated by O(N^T.T), where T is the hidden states. On the other hand the Viterbi Uses the dynamic programming solution for finding the most probable hidden states sequence.If we have a set of states Q and a set of observations O, we are trying to find the state sequence that maximizes P(Q|O).By conditional probability, we can transform P(Q|O) to P(Q,O)/P(O), but there is no need in finding P(O) as P(O) does not pertain to changes in state sequences. We can just find the state sequence that maximizes P(Q,O). Letâ€™s dive into the formula for P(Q, O).\n",
    "\n",
    "The dataset used in the above example is hindi dataset, the method generate_hmm_data inorder to get the list states and the tags, and we make a dictionary for the state_ids and observation_ids. Then we split the dataset in the train and the test samples and inorder to handle the new word we create and UNK_Token and have the len of the words corresponding the the value of the dictionary item. We then fit the dataset and predict the values and get the accuracy correspondin to the model.\n",
    "\n",
    "We also use the pickle library in the assignment inorder to serialise the pickle object, and the deserialise it inorder to use in other function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96cc70a",
   "metadata": {},
   "source": [
    "### Describe your method for the task (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0eb5ed1",
   "metadata": {},
   "source": [
    "Important details about the implementation. Feature engineering, parameter choice etc.\n",
    "\n",
    "Feature extraction refers to assignment of the weights to the features based and in this problem the weights are assigned as follows :-\n",
    "\n",
    "1. The Token ie word \n",
    "2. The previous word corrresponding to a word.\n",
    "3. and the tag of the word utilised which we get from the pos tagging using the decode function.\n",
    "\n",
    "\n",
    "The NER uses the logistic regression is the alpha which is the learning rate which is tuned using the train and dev dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa84f3f",
   "metadata": {},
   "source": [
    "### Experiment Results (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa642620",
   "metadata": {},
   "source": [
    "Typically a table summarizing all the different experiment results for various parameter choices.\n",
    "\n",
    "For CRF :- \n",
    "\n",
    "The hyperparameter tuning of the epoch from 30 to 60 showed that there was overfitting of the data on the training set and thus the F1 of the model was reduced to 0.7 for the testing dataset. Moreover if I changed the learning rate from 0.05 to 0.005 the model was take more time for the convergence. Thus, for the paramter tuning I selected out the learning rate as 0.05 and the number of the epoch as 30 inorder to attain the following results :-\n",
    "\n",
    "The F1 score obtained using this configuration is 0.7\n",
    "\n",
    "For HMM :-\n",
    "\n",
    "In case of the Hidden markov model where decoding is done using the viterbi algorithm the measures are as follows :-\n",
    "\n",
    "1. dev accuracy: 0.8031914893617021 on the dev set.\n",
    "2. test accuracy: 0.7932900432900433 on the test set.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cf3384",
   "metadata": {},
   "source": [
    "### Discussion (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f974b3",
   "metadata": {},
   "source": [
    "Key takeaway from the assignment. Why is the method good? shortcomings? how would you improve? Additional thoughts?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e457ee",
   "metadata": {},
   "source": [
    "\n",
    "The assignment helped to understand the working behind the machine learning model ie how the HMM and Viterbi are used to predict classes in case the sequence of the words are being given, HMM takes into assumption that the next state is only dependent on the previous state,\n",
    "\n",
    "\n",
    "In case of the NER algorithm the underlying algo is logisitic regression in which the training is divided into mini-batches that helped in order to attain better accuracy results. The tunning of the parameters of the alpha is done using the training set of the logistic regression so that it converges to either 0 or 1 based on the threshold of the sigmoid function. The threshold value selected here is 0.5. The logistic regression and the Naive bayes are better than the random classifier as the random classifier just provides the labels just based on the random classification of the data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "82aa5fe39a5e9fff1138eca97f8ac434374f6520b2a0e6d83f7030a43c5f3a3d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
